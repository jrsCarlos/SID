{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo multi-agente (IV)\n",
    "\n",
    "En los notebooks anteriores hemos visto los siguientes algoritmos:\n",
    "\n",
    "* Joint Action Learning with Game Theory (JAL-GT), que permite resolver juegos con diferentes conceptos de solución, en dos versiones:\n",
    "  * Valores Q en forma de matriz $N \\times S \\times AS$.\n",
    "  * Valores Q como redes neuronales, con input el estado en un vector _one-hot_ y output un valor para cada acción conjunta.\n",
    "* Joint Action Learning with Agent Modelling (JAL-AM), modelando las políticas de los demás agentes de manera probabilística, permitiendo calcular las mejores respuestas a las acciones conjuntas.\n",
    "* Win or Learn Fast, un algoritmo a medio camino entre un método basado en valor (_value-based_) y un método de gradiente de política (_policy-based_), aunque sin realmente calcular un gradiente ya que son fijos como parámetro inicial: tasa de aprendizaje cuando se pierde y tasa de aprendizaje cuando se gana.\n",
    "\n",
    "En estos dos últimos notebooks vamos a ver dos algoritmos nuevos que son de referencia en aprendizaje por refuerzo de un agente pero que también se utilizan en entornos multiagente, reduciendo el problema MARL a un problema de un agente (como aprendizaje independiente), debido a su adaptabilidad (al usar redes neuronales para representar sus elementos) y a ciertas mejoras que permiten una mayor estabilidad de aprendizaje. Esta mejora en la estabilidad permite resolver, en muchos casos, el problema de la no-estacionariedad. Sin embargo, no ofrecen ninguna garantía de convergencia, por lo que, en un caso general y especialmente en aquellos escenarios que combinen intereses similares y conflictivos a la vez, la mejor opción es la de utilizar algoritmos específicos de MARL (e.g. MAPPO o MADDPG)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Q Network\n",
    "\n",
    "El primer algoritmo que veremos es Deep Q Network. Es una evolución de Q-Learning que es, junto con Proximal Policy Optimization (PPO), el algoritmo de aprendizaje por refuerzo más usado a día de hoy.\n",
    "\n",
    "La idea fundamental de este algoritmo es reemplazar la tabla/vector/matriz de valores Q por una red neuronal, llamada modelo $Q$. El uso de redes neuronales permite la representación y ajuste de este valor en entornos continuos o con un tamaño intratable de estados.\n",
    "\n",
    "Sin embargo, un problema inherente a los algoritmos _value-based_ es la frecuente falta de estabilidad en el aprendizaje, debido a que la política se basa en los mismos valores que estamos constantemente cambiando, potencialmente causando problemas de no-estacionariedad y convergencia entre los cambios en los valores Q y el comportamiento que esos mismos cambios causan. Este problema aún se agrava más cuando, como en el caso de Q-Learning con redes neuronales, ampliamos el tamaño del espacio de estados (e.g. de discreto a continuo) o el espacio de acciones.\n",
    "\n",
    "DQN implementa una mejora que pretende solventar este problema: añadir otro modelo de valor $Q'$, de estructura idéntica a la que se está aprendiendo, que es se mantiene estática la mayor parte del tiempo y que se actualiza con los valores de $Q$, con una baja frecuencia. Este modelo $Q'$ se utiliza para calcular el _TD target_ (la diferencia temporal). Al ser un valor casi siempre estático, ayuda a mejorar la convergencia.\n",
    "\n",
    "Para mejorar la estabilidad del entrenamiento todavía más, DQN se fundamenta en una segunda mejora: el entrenamiento basado en un buffer de experiencias (donde cada experiencia es una tupla $(s, a, r, s')$) que se guardan en una cola de tamaño limitado donde se guardan las experiencias más recientes. Cada iteración, se entrena en base a una muestra aleatoria del contenido de este buffer. De esta manera, se busca evitar cambios sesgados y repentinos a los valores Q, al calcular los gradientes en base a lotes (_batches_) de experiencias en lugar de procesarlas una a una.\n",
    "\n",
    "El pseudocódigo del algoritmo DQN ([Multi-Agent Reinforcement Learning, Albrecht et al. 2024](https://www.marl-book.com/download/), Sección 9.3.1, p. 225) es:\n",
    "\n",
    "<div>\n",
    "<img src=\"dqn.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "DQN, utilizado como algoritmo de aprendizaje independiente, no ofrece garantías de convergencia a ningún concepto de solución. Sin embargo, en muchos entornos, especialmente en aquellos que ofrezcan situaciones de intereses no mixtos, permite aproximar equilibrios de Nash de manera similar a como se consigue con WoLF (ver notebook `10_wolf.ipynb`) debido a los métodos de mejora de estabilidad aplicados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalación y configuración"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dependencias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip --quiet install rlcard pettingzoo seaborn matplotlib numpy pandas tinynn pygame scikit-learn tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports necesarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tinynn.core.layer import Dense, ReLU\n",
    "from tinynn.core.loss import MSE\n",
    "from tinynn.core.model import Model\n",
    "from tinynn.core.net import Net\n",
    "from tinynn.core.optimizer import SGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Código de anteriores notebooks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pretty_print_array(ar):\n",
    "    return np.array_str(np.array(ar), precision=2, suppress_small=True)\n",
    "\n",
    "def draw_history(history, title):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(history) + 1), title: history})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y=title, data=data)\n",
    "\n",
    "    plt.title(title + ' Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a36ef3401f59ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GameModel:\n",
    "    def __init__(self, num_agents, num_actions):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = self.generate_action_space()\n",
    "        self.action_space_index = {joint_action: idx for idx, joint_action in enumerate(self.action_space)}\n",
    "\n",
    "    def generate_action_space(self):\n",
    "        actions_by_players = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            actions_by_players.append(range(self.num_actions))\n",
    "        all_joint_actions = itertools.product(*actions_by_players)\n",
    "        return [tuple(l) for l in all_joint_actions]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e1e84544dcbe33",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MARLAlgorithm(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, joint_action, rewards, state, next_state, terminated, truncated):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a80bbae53cdcc562",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algoritmo DQN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la constructora podemos ver los nuevos elementos:\n",
    "\n",
    "* Buffer de experiencias: `experience_replay`\n",
    "* Modelo $Q$: `q_value`\n",
    "* Modelo $Q'$: `q_target`\n",
    "\n",
    "Además, para el manejo del buffer de experiencias tenemos parámetros nuevos:\n",
    "\n",
    "* `buffer_size` determina el tamaño del buffer de experiencias. Si se llena, las experiencias más antiguas van desapareciendo en favor de las nuevas.\n",
    "* `batch_size` es el tamaño de la muestra que se extrae aleatoriamente del buffer para el entreno.\n",
    "* `update_steps` es la frecuencia de actualización del modelo $Q'$: el número de veces que el modelo $Q$ se ha actualizado."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQN(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel,\n",
    "                 buffer_size=4096, batch_size=512, update_steps=1024, # Parámetros nuevos\n",
    "                 gamma=0.95, alpha=0.5, epsilon=0.2, seed=42):\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "        # Elementos nuevos con respecto a Q-Learning\n",
    "        self.experience_replay = deque(maxlen=buffer_size)\n",
    "        net_q_value = Net([Dense(64), ReLU(),\n",
    "                           Dense(32), ReLU(),\n",
    "                           Dense(self.game.num_actions)])\n",
    "        net_q_target = Net([Dense(64), ReLU(),\n",
    "                            Dense(32), ReLU(),\n",
    "                            Dense(self.game.num_actions)])\n",
    "        optimizers = [SGD(lr=self.alpha) for _ in range(2)]\n",
    "        self.q_value = Model(net=net_q_value, loss=MSE(), optimizer=optimizers[0])\n",
    "        self.q_target = Model(net=net_q_target, loss=MSE(), optimizer=optimizers[1])\n",
    "        self.update_steps = update_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "        self.metrics = {\"td_error\": [], \"loss\": []}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d8f1e944fd2d16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos un método `update_q` que, dada una muestra de experiencias, calcula las diferencias temporales para cada experiencia y calcula y aplica los gradientes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def update_q(self, experience_samples):\n",
    "        # experience_samples = [experience_1, ..., experience_N]\n",
    "        # experience_n = (s, a, r, s')\n",
    "        states = np.array([e[0] for e in experience_samples])\n",
    "        next_states = np.array([e[3] for e in experience_samples])\n",
    "\n",
    "        predicted_q_values = self.q_value.forward(states)\n",
    "        target_q_values = copy.deepcopy(predicted_q_values)\n",
    "        static_max_q_values = np.max(self.q_target.forward(next_states), axis=1)\n",
    "\n",
    "        td_error = []\n",
    "        for idx, (s, a, r, next_s, terminated) in enumerate(experience_samples):\n",
    "            if terminated:\n",
    "                target_q_values[idx][a] = r\n",
    "            else:\n",
    "                target_q_values[idx][a] = r + self.gamma * static_max_q_values[idx]\n",
    "            td_error = target_q_values[idx][a] - predicted_q_values[idx][a]\n",
    "        loss, grads = self.q_value.backward(predicted_q_values, target_q_values)\n",
    "        self.q_value.apply_grads(grads)\n",
    "\n",
    "\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter > self.update_steps:\n",
    "            # Si hemos entrenado self.update_steps veces, actualizamos Q'\n",
    "            q_value_params = self.q_value.net.params\n",
    "            q_value_params_copy = copy.deepcopy(q_value_params)\n",
    "            self.q_target.net.params = q_value_params_copy\n",
    "            self.step_counter = 0  # Reseteamos el contador de actualizaciones\n",
    "\n",
    "        return np.average(td_error), loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El método `learn` registra la experiencia en el buffer y, si hay suficientes elementos para muestrear, entrenamos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def learn(self, joint_action, rewards, state, next_state, terminated, truncated):\n",
    "        experience = (state[self.agent_id], joint_action[self.agent_id],\n",
    "                      rewards[self.agent_id], next_state[self.agent_id],\n",
    "                      terminated[self.agent_id])\n",
    "        self.experience_replay.append(experience)\n",
    "        if len(self.experience_replay) >= self.batch_size:\n",
    "            experience_samples = self.rng.sample(self.experience_replay, self.batch_size)\n",
    "            td_error, loss = self.update_q(experience_samples)\n",
    "            # Guardamos el error de diferencia temporal y la pérdida de la red neuronal para estadísticas posteriores\n",
    "            self.metrics['td_error'].append(abs(td_error))\n",
    "            self.metrics['loss'].append(abs(loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, utilizamos el máximo valor Q para un estado para determinar la política en dicho estado, teniendo en cuenta que si estamos en entrenamiento aplicamos $\\epsilon$-greediness:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQN(DQN):\n",
    "    def select_action(self, state, train=True):\n",
    "        if train and self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(range(self.game.num_actions))\n",
    "        else:\n",
    "            return np.argmax(self.q_value.forward(np.array([state]))[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrenamiento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este caso vamos a trabajar con un entorno continuo: el [pursuit_v4](https://pettingzoo.farama.org/environments/sisl/pursuit/). Echad un vistazo a la documentación para entender el objetivo de entrenamiento y cómo se representa el estado y las acciones. Debido a que el estado tiene una representación matricial vamos a aplicar un `flatten` para convertirlo en un vector:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def obs_to_state(obs):\n",
    "    return obs.flatten()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8258d9410182c56",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta es la función de entrenamiento, parecida a la utilizada en notebooks anteriores. En este caso hemos introducido un cambio, que consiste en permitir definir una serie de escenarios aleatorios. Cada epoch consiste en hacer un entrenamiento completo en cada uno de los escenarios. Por lo tanto, si definimos 5 escenarios y 10 epochs, haremos un total de 50 entrenamientos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_agents(env, scenarios, epochs, gammas, alphas, epsilons, seeds):\n",
    "    env.reset()\n",
    "    agent_strings = env.agents\n",
    "    num_agents = len(env.agents)\n",
    "\n",
    "    # Crear modelo de juego y algoritmos para cada agente\n",
    "    game_model = [GameModel(num_agents=num_agents, num_actions=env.action_space(agent_strings[i]).n)\n",
    "                  for i in range(num_agents)]\n",
    "    algorithms = [DQN(agent_id, game_model[agent_id], buffer_size=50000, batch_size=128, update_steps=10000,\n",
    "                      gamma=gammas[agent_id], alpha=alphas[agent_id], epsilon=epsilons[agent_id],\n",
    "                      seed=seeds[agent_id])\n",
    "                  for agent_id in range(num_agents)]\n",
    "\n",
    "    # Métricas a guardar: recompensas y acciones\n",
    "    cumulative_rewards = [[0] * num_agents]\n",
    "    actions_played = [[]] * num_agents\n",
    "\n",
    "    for _ in tqdm(range(epochs)): # Recorridos completos a todos los escenarios\n",
    "        for idx in range(scenarios): # Escenarios aleatorios\n",
    "            # Observación inicial\n",
    "            observations, infos = env.reset(seed=idx)\n",
    "            states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "            episode_rewards = [0] * num_agents\n",
    "            while env.agents:\n",
    "                env.render()\n",
    "\n",
    "                # Selección de acciones\n",
    "                actions = []\n",
    "                petting_zoo_actions = {}\n",
    "                for i in range(num_agents):\n",
    "                    actions.append(algorithms[i].select_action(states[i]))\n",
    "                    actions_played[i].append(actions[i])\n",
    "                    petting_zoo_actions[agent_strings[i]] = actions[i]\n",
    "\n",
    "                # Actualización de entorno\n",
    "                observations, rewards, terminations, truncations, infos = env.step(petting_zoo_actions)\n",
    "\n",
    "                # Conversión de formato petting zoo -> lista\n",
    "                next_states = []\n",
    "                indexed_rewards = []\n",
    "                indexed_terminations = []\n",
    "                indexed_truncations = []\n",
    "                for i in range(num_agents):\n",
    "                    next_states.append(obs_to_state(observations[agent_strings[i]]))\n",
    "                    indexed_rewards.append(rewards[agent_strings[i]])\n",
    "                    indexed_terminations.append(terminations[agent_strings[i]])\n",
    "                    indexed_truncations.append(truncations[agent_strings[i]])\n",
    "                    episode_rewards[i] += indexed_rewards[i]\n",
    "\n",
    "                # Entrenamiento\n",
    "                for i in range(num_agents):\n",
    "                    algorithms[i].learn(actions, indexed_rewards, states, next_states,\n",
    "                                        indexed_terminations, indexed_truncations)\n",
    "                states = next_states\n",
    "            cumulative_rewards.append([cumulative_rewards[-1][i] + indexed_rewards[i]\n",
    "                                       for i in range(num_agents)])\n",
    "            env.close()\n",
    "    return game_model, algorithms, cumulative_rewards, actions_played"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd2be3caba89ee7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuración y ejecución del entrenamiento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este caso comenzamos con algunos valores pequeños para muchos parámetros, menores que los que se muestran en la documentación.\n",
    "\n",
    "**Antes de entrenar, intentad predecir el efecto en el entrenamiento que tendrá cada uno de los parámetros.** Pensad que la complejidad del entrenamiento es mucho mayor que la que hemos podido ver en anteriores notebooks, ya sea en entrenamiento de un agente o multiagente. Los entrenamientos pueden ser lentos, por lo que una exploración a ciegas puede ser bastante ineficiente.\n",
    "\n",
    "Si queréis ver cómo se comportan los agentes, poned `render_mode=\"human\"`, aunque puede que vaya algo más lento."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pettingzoo.sisl import pursuit_v4\n",
    "\n",
    "env = pursuit_v4.parallel_env(render_mode=\"rgb_array\", x_size=8, y_size=8,\n",
    "                              n_evaders=1, n_pursuers=2, surround=False,\n",
    "                              obs_range=2, max_cycles=50)\n",
    "env.reset()\n",
    "num_agents = len(env.agents)\n",
    "game_model, algorithms, cumulative_rewards, actions_played = \\\n",
    "    train_agents(env, scenarios=10, epochs=10, gammas=[0.95] * num_agents,\n",
    "                 alphas=[0.1] * num_agents, epsilons=[0.2] * num_agents,\n",
    "                 seeds=[i for i in range(num_agents)])\n",
    "\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_history([cumulative_rewards[idx][0] for idx in range(len(cumulative_rewards))], \"Rewards\")\n",
    "draw_history(algorithms[0].metrics['td_error'], \"TD Error\")\n",
    "draw_history(algorithms[0].metrics['loss'], \"Loss\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluación\n",
    "\n",
    "Con este código podéis probar escenarios aleatorios. **Comprobad si los algoritmos que habéis entrenado han generalizado lo suficiente como para poder resolver escenarios nuevos.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = pursuit_v4.parallel_env(render_mode=\"human\", x_size=8, y_size=8,\n",
    "                              n_evaders=1, n_pursuers=2, surround=False,\n",
    "                              obs_range=2, max_cycles=1000)\n",
    "observations, infos = env.reset(seed=random.randint(0, 10000))\n",
    "agent_strings = env.agents\n",
    "all_rewards = [0]\n",
    "\n",
    "states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "while env.agents:\n",
    "    env.render()\n",
    "    actions = [algorithms[i].select_action(states[i]) for i in range(num_agents)]\n",
    "    petting_zoo_actions = {agent_strings[i]: actions[i] for i in range(num_agents)}\n",
    "    observations, rewards, terminations, truncations, infos = env.step(petting_zoo_actions)\n",
    "    next_states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "    indexed_rewards = [rewards[agent_strings[i]] for i in range(num_agents)]\n",
    "    indexed_terminations = [terminations[agent_strings[i]] for i in range(num_agents)]\n",
    "    all_rewards.append(all_rewards[-1] + sum(indexed_rewards))\n",
    "    states = next_states\n",
    "if any(indexed_terminations):\n",
    "    print(\"Todos los evaders han sido cazados!\")\n",
    "env.close()\n",
    "draw_history(all_rewards, \"Cumulative reward\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
