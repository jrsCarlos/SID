{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo multi-agente (y V)\n",
    "\n",
    "Habíamos visto en WoLF (`10_wolf.ipynb`) como los dos enfoques _value-based_ y _policy-based_ se pueden utilizar juntos para ofrecer una mayor estabilidad de entrenamiento. Aunque WoLF facilita ciertas mejoras en el rendimiento, carece de un cálculo real de gradientes, lo que puede resultar en una convergencia más lenta o no subóptima.\n",
    "\n",
    "En este notebook, daremos un paso más allá introduciendo los algoritmos Actor-Critic, una familia de métodos que eficazmente integran los enfoques value-based y policy-based mediante el uso de gradientes para la optimización de políticas. Uno de los representantes más destacados y eficientes de esta familia es el Advantage Actor-Critic (A2C).\n",
    "\n",
    "### Algoritmo A2C\n",
    "\n",
    "El algoritmo A2C utiliza dos componentes principales:\n",
    "\n",
    "* Actor: se encarga de determinar la acción a tomar, basándose en una política parametrizada $\\pi(a | s; \\theta)$.\n",
    "* Crítico: proporciona una evaluación (función de valor $V$ o $Q$, dependiendo del algoritmo) de las acciones realizadas por el actor, calculando el valor de estado o el valor del estado-acción, y genera un valor de error o ventaja (llamado, genéricamente, _advantage_).\n",
    "\n",
    "En base a estos dos componentes, el algoritmo A2C calcula, tras cada episodio, las ventajas que ha obtenido el actor, con cada una de sus acciones, según el valor otorgado por el crítico. En base a estas ventajas, se calcula el gradiente de la política y se modifica el modelo de política. Tanto el cálculo de la ventaja como el cálculo del gradiente de política se pueden hacer de diferentes maneras, pero las más estándar son:\n",
    "\n",
    "* Para el cálculo de la ventaja, se suele usar la diferencia temporal $G_t = r_t + \\gamma * V(s_{t+1}) - V(s_t)$, donde $V$ es el modelo del crítico.\n",
    "* Para el cálculo del gradiente, se suele usar la fórmula de pérdida $L(\\theta)$ que se utiliza en REINFORCE y que hemos visto en teoría (tema 6 de teoría, p. 38 o notebook `05_reinforce.ipynb`), adaptada al cálculo de la ventaja del punto anterior: $-\\frac{1}{T} \\sum_t^{T} G_t \\cdot log \\pi(a_t | s_t; \\theta) $\n",
    "\n",
    "La motivación principal es intentar mejorar la estabilidad al evitar que todo el aprendizaje se base en la modificación constante de un único valor (ya sea $V$, $Q$ o $\\pi$), a partir del cual se obtiene el comportamiento entrenado. Ya hemos discutido en notebooks anteriores cómo DQN utiliza dos valores $Q$ para evitar este problema, o cómo WoLF permite salvar el problema de la no-estacionariedad al mantener, a la vez, una política, una política media y un valor Q. En el caso de los algoritmos Actor-Critic, la propuesta es tener un valor $Q$ o $V$, además de una política, interactuando constantemente.\n",
    "\n",
    "En muchos entornos multiagente y de manera similar a como pasa con DQN y con similares limitaciones, los algoritmos Actor-Critic son adecuados para el aprendizaje por refuerzo, tanto en entornos puramente cooperativos como puramente competitivos. En situaciones de intereses mixtos la convergencia a un equilibrio no está garantizada.\n",
    "\n",
    "El pseudocódigo del algoritmo DQN ([Multi-Agent Reinforcement Learning, Albrecht et al. 2024](https://www.marl-book.com/download/), Sección 8.2.5, p. 205) es:\n",
    "\n",
    "<div>\n",
    "<img src=\"a2c.png\" width=\"500\"/>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalación y configuración"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dependencias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip --quiet install rlcard pettingzoo seaborn matplotlib numpy pandas tinynn pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports necesarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tinynn.core.layer import Dense, ReLU\n",
    "from tinynn.core.loss import MSE\n",
    "from tinynn.core.model import Model\n",
    "from tinynn.core.net import Net\n",
    "from tinynn.core.optimizer import SGD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06b5e8e96d10244",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Código de anteriores notebooks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pretty_print_array(ar):\n",
    "    return np.array_str(np.array(ar), precision=2, suppress_small=True)\n",
    "\n",
    "def draw_history(history, title):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(history) + 1), title: history})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y=title, data=data)\n",
    "\n",
    "    plt.title(title + ' Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    return obs.flatten()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a36ef3401f59ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GameModel:\n",
    "    def __init__(self, num_agents, num_actions):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = self.generate_action_space()\n",
    "        self.action_space_index = {joint_action: idx for idx, joint_action in enumerate(self.action_space)}\n",
    "\n",
    "    def generate_action_space(self):\n",
    "        actions_by_players = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            actions_by_players.append(range(self.num_actions))\n",
    "        all_joint_actions = itertools.product(*actions_by_players)\n",
    "        return [tuple(l) for l in all_joint_actions]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e1e84544dcbe33",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MARLAlgorithm(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, joint_action, rewards, state, next_state, terminated, truncated):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a80bbae53cdcc562",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algoritmo A2C\n",
    "\n",
    "En la constructora podéis ver los componentes principales:\n",
    "\n",
    "* El modelo de actor: una red neuronal con tantas salidas como acciones (política $\\pi$).\n",
    "* El modelo de crítico: una red neuronal con una salida (valor de estado $V$).\n",
    "* Un buffer donde almacenar todas las experiencias del episodio activo (como en REINFORCE)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class A2C(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel,\n",
    "                 gamma=0.95, alpha_actor=0.5, alpha_critic=0.5, epsilon=0.2, seed=42):\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.alpha_actor = alpha_actor\n",
    "        self.alpha_critic = alpha_critic\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "        net_actor = Net([Dense(64), ReLU(),\n",
    "                         Dense(32), ReLU(),\n",
    "                         Dense(self.game.num_actions)])\n",
    "        optimizer_actor = SGD(lr=self.alpha_actor)\n",
    "        net_critic = Net([Dense(64), ReLU(),\n",
    "                          Dense(32), ReLU(),\n",
    "                          Dense(1)])\n",
    "        optimizer_critic = SGD(lr=self.alpha_critic)\n",
    "        self.actor = Model(net=net_actor, loss=MSE(), optimizer=optimizer_actor)\n",
    "        self.critic = Model(net=net_critic, loss=MSE(), optimizer=optimizer_critic)\n",
    "        self.episode_buffer = []\n",
    "        self.metrics = {\"actor_loss\": [], \"critic_loss\": [], \"advantage\": []}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d8f1e944fd2d16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "El método `compute_advantages` calcula la ventaja para una trayectoria completa, generalmente un episodio terminado. Como en REINFORCE, en este caso se usan las diferencias temporales de cada paso y se calculan de último a primer paso para ir acumulando los descuentos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class A2C(A2C):\n",
    "    def compute_advantages(self, rewards, values, next_values, terminated):\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if terminated[t]:\n",
    "                advantages[t] = rewards[t] - values[t]\n",
    "            else:\n",
    "                advantages[t] = rewards[t] + self.gamma * next_values[t] - values[t]\n",
    "        return advantages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El método `update_actor_and_critic` se llama al final de cada episodio y se encarga de entrenar los dos modelos de actor y crítico:\n",
    "\n",
    "* En primer lugar se obtienen los valores ($V(s_t)$) y los siguientes valores ($V(s_{t+1})$) para cada paso $t$.\n",
    "* A partir de estos valores y la recompensa, se calculan las ventajas $G_t$.\n",
    "* Luego se actualiza el crítico, calculando los gradientes en base a los valores originales más la ventaja correspondiente a cada paso.\n",
    "* Finalmente se actualiza el actor en base a la fórmula de gradiente de política vista en REINFORCE: $-G_t \\cdot log(\\pi(a_t | s_t))$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class A2C(A2C):\n",
    "    def update_actor_and_critic(self):\n",
    "        states = np.array([e[0] for e in self.episode_buffer])\n",
    "        actions = np.array([e[1] for e in self.episode_buffer])\n",
    "        rewards = np.array([e[2] for e in self.episode_buffer])\n",
    "        next_states = np.array([e[3] for e in self.episode_buffer])\n",
    "        terminated = np.array([e[4] for e in self.episode_buffer])\n",
    "\n",
    "        values = self.critic.forward(states)\n",
    "        next_values = self.critic.forward(next_states)\n",
    "        advantages = self.compute_advantages(rewards, values.flatten(), next_values.flatten(), terminated)\n",
    "\n",
    "        # Actualizar crítico\n",
    "        target_values = values + [[a] for a in advantages]\n",
    "        critic_loss, critic_grads = self.critic.backward(values, target_values)\n",
    "        self.critic.apply_grads(critic_grads)\n",
    "\n",
    "        # Actualizar actor\n",
    "        policies = [softmax(policy) for policy in self.actor.forward(states)]\n",
    "        target_policies = copy.deepcopy(policies)\n",
    "        for t in range(len(self.episode_buffer)):\n",
    "            G_t = advantages[t]\n",
    "            action_probs = policies[t]\n",
    "            policy_gradient = -G_t * np.log(action_probs[actions[t]])\n",
    "            target_policies[t][actions[t]] += policy_gradient\n",
    "        actor_loss, actor_grads = self.actor.backward(np.array(policies), np.array(target_policies))\n",
    "        self.actor.apply_grads(actor_grads)\n",
    "\n",
    "        return actor_loss, critic_loss, np.mean(advantages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El método de aprendizaje añade la experiencia al buffer del episodio. Si todos los agentes han terminado o han sido cortados (e.g. por tiempo), entonces entrenamos y vaciamos el buffer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class A2C(A2C):\n",
    "    def learn(self, joint_action, rewards, state, next_state, terminated, truncated):\n",
    "        experience = (state[self.agent_id], joint_action[self.agent_id],\n",
    "                      rewards[self.agent_id], next_state[self.agent_id],\n",
    "                      terminated[self.agent_id])\n",
    "        self.episode_buffer.append(experience)\n",
    "        if all([terminated[i] or truncated[i] for i in range(len(terminated))]):\n",
    "            actor_loss, critic_loss, advantage = self.update_actor_and_critic()\n",
    "            self.episode_buffer = []\n",
    "            # Guardamos ventaja acumulada y las pérdidas de actor y crítico\n",
    "            self.metrics['actor_loss'].append(abs(actor_loss))\n",
    "            self.metrics['critic_loss'].append(abs(critic_loss))\n",
    "            self.metrics['advantage'].append(advantage)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, la función que representa a la política es diferente a la de Q-Learning. En gradiente de política no hay $\\epsilon$-greediness ya que la política permite exploración siempre y cuando los valores no converjan abruptamente a 1s y 0s:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class A2C(A2C):\n",
    "    def select_action(self, state):\n",
    "        np.random.seed(self.rng.randint(0, 10000))\n",
    "        return np.random.choice(range(self.game.num_actions), p=softmax(self.actor.forward(np.array([state]))[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrenamiento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_agents(env, scenarios, epochs, gammas, alphas, epsilons, seeds):\n",
    "    env.reset()\n",
    "    agent_strings = env.agents\n",
    "    num_agents = len(env.agents)\n",
    "\n",
    "    # Crear modelo de juego y algoritmos para cada agente\n",
    "    game_model = [GameModel(num_agents=num_agents, num_actions=env.action_space(agent_strings[i]).n)\n",
    "                  for i in range(num_agents)]\n",
    "    algorithms = [A2C(agent_id, game_model[agent_id], gamma=gammas[agent_id],\n",
    "                      alpha_actor=alphas[agent_id], alpha_critic=alphas[agent_id],\n",
    "                      epsilon=epsilons[agent_id],\n",
    "                      seed=seeds[agent_id])\n",
    "                  for agent_id in range(num_agents)]\n",
    "\n",
    "    # Métricas a guardar: recompensas y acciones\n",
    "    cumulative_rewards = [[0] * num_agents]\n",
    "    actions_played = [[]] * num_agents\n",
    "\n",
    "    for _ in tqdm(range(epochs)): # Recorridos completos a todos los escenarios\n",
    "        for idx in range(scenarios): # Escenarios aleatorios\n",
    "            # Observación inicial\n",
    "            observations, infos = env.reset(seed=idx)\n",
    "            states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "            episode_rewards = [0] * num_agents\n",
    "            while env.agents:\n",
    "                env.render()\n",
    "\n",
    "                # Selección de acciones\n",
    "                actions = []\n",
    "                petting_zoo_actions = {}\n",
    "                for i in range(num_agents):\n",
    "                    actions.append(algorithms[i].select_action(states[i]))\n",
    "                    actions_played[i].append(actions[i])\n",
    "                    petting_zoo_actions[agent_strings[i]] = actions[i]\n",
    "\n",
    "                # Actualización de entorno\n",
    "                observations, rewards, terminations, truncations, infos = env.step(petting_zoo_actions)\n",
    "\n",
    "                next_states = []\n",
    "                indexed_rewards = []\n",
    "                indexed_terminations = []\n",
    "                indexed_truncations = []\n",
    "                for i in range(num_agents):\n",
    "                    # Conversión de formato petting zoo -> lista\n",
    "                    next_states.append(obs_to_state(observations[agent_strings[i]]))\n",
    "                    indexed_rewards.append(rewards[agent_strings[i]])\n",
    "                    indexed_terminations.append(terminations[agent_strings[i]])\n",
    "                    indexed_truncations.append(truncations[agent_strings[i]])\n",
    "                    episode_rewards[i] += indexed_rewards[i]\n",
    "                for i in range(num_agents):\n",
    "                    # Entrenamiento\n",
    "                    algorithms[i].learn(actions, indexed_rewards, states, next_states,\n",
    "                                        indexed_terminations, indexed_truncations)\n",
    "                states = next_states\n",
    "            cumulative_rewards.append([cumulative_rewards[-1][i] + indexed_rewards[i]\n",
    "                                       for i in range(num_agents)])\n",
    "            env.close()\n",
    "    return game_model, algorithms, cumulative_rewards, actions_played"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f88201cec6e17e91",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pettingzoo.sisl import pursuit_v4\n",
    "\n",
    "env = pursuit_v4.parallel_env(render_mode=\"rgb_array\", x_size=8, y_size=8,\n",
    "                              n_evaders=1, n_pursuers=2, surround=False,\n",
    "                              obs_range=2, max_cycles=50)\n",
    "env.reset()\n",
    "num_agents = len(env.agents)\n",
    "game_model, algorithms, cumulative_rewards, actions_played = \\\n",
    "    train_agents(env, scenarios=10, epochs=10, gammas=[0.95] * num_agents, alphas=[0.1] * num_agents,\n",
    "                 epsilons=[0.2] * num_agents, seeds=[i for i in range(num_agents)])\n",
    "\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_history([cumulative_rewards[idx][0] for idx in range(len(cumulative_rewards))], \"Rewards\")\n",
    "draw_history(algorithms[0].metrics['advantage'], \"Advantage\")\n",
    "draw_history(algorithms[0].metrics['actor_loss'], \"Actor loss\")\n",
    "draw_history(algorithms[0].metrics['critic_loss'], \"Critic loss\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluación"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = pursuit_v4.parallel_env(render_mode=\"human\", x_size=8, y_size=8,\n",
    "                              n_evaders=1, n_pursuers=2, surround=False,\n",
    "                              obs_range=2, max_cycles=1000)\n",
    "observations, infos = env.reset(seed=random.randint(0, 10000))\n",
    "agent_strings = env.agents\n",
    "all_rewards = [0]\n",
    "\n",
    "states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "while env.agents:\n",
    "    env.render()\n",
    "    actions = [algorithms[i].select_action(states[i]) for i in range(num_agents)]\n",
    "    petting_zoo_actions = {agent_strings[i]: actions[i] for i in range(num_agents)}\n",
    "    observations, rewards, terminations, truncations, infos = env.step(petting_zoo_actions)\n",
    "    next_states = [obs_to_state(observations[agent_strings[i]]) for i in range(num_agents)]\n",
    "    indexed_rewards = [rewards[agent_strings[i]] for i in range(num_agents)]\n",
    "    indexed_terminations = [terminations[agent_strings[i]] for i in range(num_agents)]\n",
    "    all_rewards.append(all_rewards[-1] + sum(indexed_rewards))\n",
    "    states = next_states\n",
    "if any(indexed_terminations):\n",
    "    print(\"Todos los evaders han sido cazados!\")\n",
    "env.close()\n",
    "draw_history(all_rewards, \"Cumulative reward\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
