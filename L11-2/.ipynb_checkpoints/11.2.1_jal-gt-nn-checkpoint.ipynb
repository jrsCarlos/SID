{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Lab: Aprendizaje por refuerzo multi-agente (II)\n",
    "\n",
    "## Joint Action Learning with Game Theory, versión con redes neuronales\n",
    "\n",
    "Un problema de la implementación de JAL-GT que vimos en la sesión anterior surge cuando tenemos un espacio de estados muy grande o continuo, o cuando queremos usar un histórico de observaciones como estado (como se define de forma abstracta la resolución teórica de un POSG).\n",
    "\n",
    "Por ejemplo, en la práctica 3 de laboratorio (POGEMA) se propone, de partida, un espacio de observaciones de 3 matrices de booleanos de tamaño 9, que dan lugar a $2^{27}$ posibles observaciones. Esto se agravaría más si para tener una representación que pueda contener información histórica sobre el mapa (por ejemplo, para salir de un laberinto) necesitaríamos considerar un histórico de observaciones, haciendo que este espacio de estados crezca de manera exponencial.\n",
    "\n",
    "La implementación de JAL-GT que hemos visto en la sesión anterior utiliza matrices para representar el valor Q. Sin embargo, no es necesario guardar la matriz completa con representación para todos los posibles estados para poder extraer información relevante para construir la política. Por ejemplo, puede que haya partes del espacio de estado que no sean importantes (por ejemplo, la casilla central en la matriz de agentes en POGEMA), o puede que haya elementos del estado que tenga sentido agrupar para extraer información importante (por ejemplo, para saber si no podemos movernos a una casilla no hace falta diferenciar entre obstáculo y agente).\n",
    "\n",
    "Una manera de aligerar el espacio de estados es hacer _feature engineering_: extraer características del estado que sabemos que son relevantes y de esta manera comprimir la información. Por ejemplo, podríamos considerar que el agente está mirando siempre hacia el objetivo y podríamos reducir las observaciones que tenemos en cuenta sólo a aquellas que están en esa dirección.\n",
    "\n",
    "Hay una manera automática de extraer estas características de manera automática: las redes neuronales, que permiten centrarse en técnicas denominadas como aprendizaje por refuerzo profundo (_Deep Reinforcement Learning_). La idea consiste en enviar las observaciones como entrada de la red neuronal y tener como salida, por ejemplo, el valor Q para cada acción, o directamente la política.\n",
    "\n",
    "En esta sesión vamos a ver cómo aplicar esta técnica al algoritmo JAL-GT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Instalación y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip --quiet install rlcard pettingzoo seaborn matplotlib numpy pandas tinynn pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pettingzoo.classic import rps_v2\n",
    "from tinynn.core.layer import Dense, ReLU\n",
    "from tinynn.core.loss import MSE\n",
    "from tinynn.core.model import Model\n",
    "from tinynn.core.net import Net\n",
    "from tinynn.core.optimizer import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Código de anteriores sesiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "RPS_CHOICES = [\"Rock\", \"Paper\", \"Scissors\"]\n",
    "PD_CHOICES = [\"Cooperate\", \"Defect\"]\n",
    "\n",
    "def pretty_print_array(ar):\n",
    "    return np.array_str(np.array(ar), precision=2, suppress_small=True)\n",
    "\n",
    "def draw_history(history, title):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(history) + 1), title: history})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y=title, data=data)\n",
    "\n",
    "    plt.title(title + ' Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class GameModel:\n",
    "    def __init__(self, num_agents, num_states, num_actions):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = self.generate_action_space()\n",
    "        self.action_space_index = {joint_action: idx for idx, joint_action in enumerate(self.action_space)}\n",
    "\n",
    "    def generate_action_space(self):\n",
    "        actions_by_players = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            actions_by_players.append(range(self.num_actions))\n",
    "        all_joint_actions = itertools.product(*actions_by_players)\n",
    "        return [tuple(l) for l in all_joint_actions]\n",
    "\n",
    "\n",
    "class SolutionConcept(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def solution_policy(self, agent_id, state, game, q_models):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MARLAlgorithm(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, joint_action, rewards, state, next_state):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Redes neuronales\n",
    "\n",
    "Utilizaremos una librería en Python que usa Numpy para calcular los gradientes. No usa GPU y no es tan eficiente ni tan potente como las librerías más usadas como Torch, Tensorflow o JAX pero ocupa mucho menos espacio de instalación.\n",
    "\n",
    "Ejemplo de cómo declarar una red neuronal con una capa de entrada de 1 neurona, una capa oculta con 2 y una capa de salida con 1 neurona, usando ReLU como función de activación, con un _learning rate_ fijo de 0.001 y usando error de media de errores cuadrados como función de pérdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "net = Net([Dense(1), ReLU(),\n",
    "           Dense(2), ReLU(),\n",
    "           Dense(1)])\n",
    "model = Model(net=net, loss=MSE(), optimizer=SGD(0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Ahora podemos usar esta red para entrenar, por ejemplo, una regresión lineal $y = 2 \\cdot x + 1$ (le añadimos un poco de error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.]]), array([[0.]]), array([[0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[4], [8], [10], [20]]\n",
    "y = [[9.1], [16.8], [20.5], [41.1]]\n",
    "\n",
    "predictions = [model.forward(np.array([x])) for x in X]  # .forward recibe un \"batch\"\n",
    "predictions  # y devuelve un batch de predicciones por cada batch de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lo que predice en este punto no es relevante ya que la red aún no ha sido entrenada. La entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 41.404999999999994\n",
      "Loss: 141.12\n",
      "Loss: 210.125\n",
      "Loss: 844.605\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    target = np.array([y[i]])\n",
    "    loss, gradients = model.backward(predictions[i], target)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    model.apply_grads(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.0875]]), array([[0.0875]]), array([[0.0875]]), array([[0.0875]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [model.forward(np.array([x])) for x in X]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Podríamos decir que ahora se ha pasado de vueltas. Podemos iterar hasta que la pérdida baje a una cantidad decente (por ejemplo, por debajo de 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "[4]\n",
      "[9.1]\n",
      "[8]\n",
      "[16.8]\n",
      "[10]\n",
      "[20.5]\n",
      "[20]\n",
      "[41.1]\n",
      "Loss: 807.9190708651736\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i in range(len(X)):\n",
    "        print(X[i])\n",
    "        print(y[i])\n",
    "        prediction = model.forward(np.array([X[i]]))\n",
    "        target = np.array([y[i]])\n",
    "        loss, gradients = model.backward(prediction, target)\n",
    "        if loss > 1:\n",
    "            model.apply_grads(gradients)\n",
    "        else:\n",
    "            break\n",
    "    if loss < 1:\n",
    "        break\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Podemos inspeccionar los pesos (w) y los sesgos (b) de cada neurona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[-0.43037045]]), array([0.]),\n",
       "       array([[-0.2590569 ,  1.38151383]]), array([0., 0.]),\n",
       "       array([[ 0.89978379],\n",
       "              [-0.71103227]]), array([0.94270824])], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Comprobemos que se ajusta bien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.94270824]]),\n",
       " array([[0.94270824]]),\n",
       " array([[0.94270824]]),\n",
       " array([[0.94270824]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [model.forward(np.array([x])) for x in X]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Y que podemos predecir cualquier valor con cierto margen de error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94270824],\n",
       "       [0.94270824]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(np.array([[200], [500]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Aplicando redes neuronales a JAL-GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lo primero que necesitaremos será poder tratar un estado como un vector, para poder pasarlo como entrada a una red neuronal. Para ello usaremos el método de _one-hot encoding_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def one_hot(index, total_length):\n",
    "    one_hot_vector = np.zeros(total_length)\n",
    "    one_hot_vector[index] = 1\n",
    "    return np.array([one_hot_vector], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Adaptando el algoritmo\n",
    "\n",
    "Primero vamos a modificar el algoritmo. En la constructora, en vez de una matriz $N \\times S \\times AS$ (agentes x estados x acciones conjuntas), vamos a usar una lista de redes neuronales, una por cada agente del cual estamos modelando su valor Q.\n",
    "\n",
    "Algunos métodos, como la función para obtener la política a partir de la solución de concepto (`update_policy`), apenas cambian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALGTNN(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel, solution_concept: SolutionConcept,\n",
    "                 gamma=0.95, alpha=0.5, epsilon=0.2, seed=42):\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.solution_concept = solution_concept\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = random.Random(seed)\n",
    "        # Política conjunta por defecto: distribución uniforme respecto\n",
    "        # de las acciones conjuntas, para cada acción (pi(a | s))\n",
    "        self.joint_policy = np.ones((self.game.num_agents, self.game.num_states,\n",
    "                                     self.game.num_actions)) / self.game.num_actions\n",
    "        self.metrics = {\"td_error\": [], \"loss\": []}\n",
    "\n",
    "        # Red neuronal: 64 neuronas de entrada, 32 en la capa oculta,\n",
    "        # |AS| (cardinalidad del espacio de acciones conjuntas) en la capa de salida\n",
    "        # Es decir, un valor Q de salida para cada acción conjunta\n",
    "        net = [Net([Dense(64), ReLU(),\n",
    "                    Dense(32), ReLU(),\n",
    "                    Dense(len(self.game.action_space))])\n",
    "               for _ in range(self.game.num_agents)]\n",
    "        optimizers = [SGD(lr=self.alpha) for _ in range(self.game.num_agents)]\n",
    "        self.q_models = [Model(net=net[i], loss=MSE(), optimizer=optimizers[i])\n",
    "                         for i in range(self.game.num_agents)]\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update_policy(self, agent_id, state):\n",
    "        self.joint_policy[agent_id][state] = self.solution_concept.solution_policy(agent_id, state, self.game, self.q_models)\n",
    "\n",
    "    def solve(self, agent_id, state):\n",
    "        return self.joint_policy[agent_id][state]\n",
    "\n",
    "    def select_action(self, state, train=True):\n",
    "        if train and self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(range(self.game.num_actions))\n",
    "        else:\n",
    "            probs = self.solve(self.agent_id, state)\n",
    "            np.random.seed(self.rng.randint(0, 10000))\n",
    "            return np.random.choice(range(self.game.num_actions), p=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Cambiamos la función de valor de estado para un agente para utilizar la predicción (`.forward()`) de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALGTNN(JALGTNN):\n",
    "    def value(self, agent_id, state):\n",
    "        value = 0\n",
    "        for idx, joint_action in enumerate(self.game.action_space):\n",
    "            payoff = self.q_models[agent_id].forward(one_hot(state, self.game.num_states))[0][idx]\n",
    "            joint_probability = np.prod([self.joint_policy[i][state][joint_action[i]]\n",
    "                                         for i in range(self.game.num_agents)])\n",
    "            value += payoff * joint_probability\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Añadimos un método para encapsular la lógica de actualización de la red neuronal para el valor Q de un agente para un estado, dados un valor para el estado siguiente (función $Value$ que vimos en la sesión anterior) y la recompensa inmediata.\n",
    "\n",
    "Los pasos con una red neuronal son los siguientes:\n",
    "\n",
    "* Obtenemos las predicciones actuales para el valor $Q(s, a)$.\n",
    "* Calculamos el objetivo de diferencia temporal (_TD target_), que es el componente que según la fórmula de actualización en el algoritmo de diferencias temporales es la recompensa más el valor futuro descontado.\n",
    "* En una copia de las predicciones, modificamos el valor correspondiente a la acción conjunta recién ejecutada para poner el valor objetivo.\n",
    "* Dejamos que la librería de redes neuronales calcule los gradientes a modificar en las neuronas de la red. Este proceso es análogo a la resta $r + \\gamma\\cdot value - Q(s,a)$.\n",
    "* Aplicamos estos gradientes a la red. Implícitamente se aplica la tasa de aprendizaje, por lo que es un proceso análogo a $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma\\cdot value - Q(s,a)\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALGTNN(JALGTNN):\n",
    "    def update_q(self, q_model, state, action, reward, next_value):\n",
    "        prediction = q_model.forward(state)[0]\n",
    "        agent_q_value = prediction[action]\n",
    "        td_target = reward + self.gamma * next_value\n",
    "        td_error = td_target - agent_q_value\n",
    "        target = prediction.copy()\n",
    "        target[action] = td_target\n",
    "        loss, grads = q_model.backward(np.array([prediction]), np.array([target]))\n",
    "        q_model.apply_grads(grads)\n",
    "        return td_error, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Modificamos el método `learn` para utilizar el método de actualización de Q que acabamos de ver. Es importante hacer la conversión del estado en su representación vectorial (e.g. el estado 3 en un entorno con 4 estados sería `[0 0 0 1]`, o el estado 0 en un entorno con 1 estado como un juego en forma normal sería `[1]`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALGTNN(JALGTNN):\n",
    "    def learn(self, joint_action, rewards, state, next_state):\n",
    "        joint_action_index = self.game.action_space_index[joint_action]\n",
    "        for agent_id in range(self.game.num_agents):\n",
    "            agent_reward = rewards[agent_id]\n",
    "            agent_game_value_next_state = self.value(agent_id, next_state)\n",
    "            one_hot_state = one_hot(state, self.game.num_states)\n",
    "            td_error, loss = self.update_q(self.q_models[agent_id], one_hot_state,\n",
    "                                           joint_action_index, agent_reward,\n",
    "                                           agent_game_value_next_state)\n",
    "            self.update_policy(agent_id, state)\n",
    "            # Guardamos el error de diferencia temporal y la pérdida de la red neuronal para estadísticas posteriores\n",
    "            self.metrics['td_error'].append(abs(td_error))\n",
    "            self.metrics['loss'].append(abs(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Con esto ya tenemos un algoritmo JAL-GT adaptado para poder utilizar espacios de estados arbitrariamente grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Conceptos de solución\n",
    "\n",
    "Como a los conceptos de solución, en la implementación de la sesión anterior, pasábamos la matriz Q, ahora tenemos que adaptarlos para utilizar la red neuronal, usando `.forward` donde sea necesario obtener los valores Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Pareto-eficiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ParetoSolutionConcept(SolutionConcept):\n",
    "    def is_dominated(self, joint_action, other_joint_action, scalar_state, game, q_models):\n",
    "        strictly_better_for_at_least_one = False\n",
    "        state = one_hot(scalar_state, game.num_states)\n",
    "        for agent_id in range(game.num_agents):\n",
    "            joint_action_index = game.action_space_index[joint_action]\n",
    "            other_joint_action_index = game.action_space_index[other_joint_action]\n",
    "\n",
    "            q_state = q_models[agent_id].forward(state)[0]\n",
    "            q_state_a_i = q_state[joint_action_index]\n",
    "            q_state_a_j = q_state[other_joint_action_index]\n",
    "\n",
    "            if q_state_a_j < q_state_a_i:\n",
    "                return False\n",
    "            if q_state_a_j > q_state_a_i:\n",
    "                strictly_better_for_at_least_one = True\n",
    "        return strictly_better_for_at_least_one\n",
    "\n",
    "    def find_pareto_efficient_solutions(self, state, game, q_models):\n",
    "        joint_actions = list(itertools.product(range(game.num_actions), repeat=game.num_agents))\n",
    "        pareto_solutions = []\n",
    "\n",
    "        for joint_action in joint_actions:\n",
    "            dominated = False\n",
    "            for other_joint_action in joint_actions:\n",
    "                if other_joint_action == joint_action:\n",
    "                    continue\n",
    "                if self.is_dominated(joint_action, other_joint_action, state, game, q_models):\n",
    "                    dominated = True\n",
    "                    break\n",
    "            if not dominated:\n",
    "                pareto_solutions.append(joint_action)\n",
    "\n",
    "        return pareto_solutions\n",
    "\n",
    "    def solution_policy(self, agent_id, state, game, q_models):\n",
    "        pareto_solutions = self.find_pareto_efficient_solutions(state, game, q_models)\n",
    "        if len(pareto_solutions) > 0:\n",
    "            action_counts = np.zeros(game.num_actions)\n",
    "            for pareto_solution in pareto_solutions:\n",
    "                action_counts[pareto_solution[agent_id]] += 1\n",
    "            probs = action_counts / len(pareto_solutions)\n",
    "            return probs\n",
    "        else:\n",
    "            uniform_distribution = np.ones(game.num_actions)\n",
    "            return uniform_distribution / np.sum(uniform_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Minimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MinimaxSolutionConcept(SolutionConcept):\n",
    "    def opponent_max_values(self, agent_id, scalar_state, game, q_models):\n",
    "        action_scores = []\n",
    "        q_opponent = q_models[1 - agent_id]\n",
    "        state = one_hot(scalar_state, game.num_states)\n",
    "        for action in range(game.num_actions):\n",
    "            max_opponent_payoff = -999999999999\n",
    "            for opponent_action in range(game.num_actions):\n",
    "                if agent_id == 0:  # Suponemos sólo dos agentes\n",
    "                    joint_action = (action, opponent_action)\n",
    "                else:\n",
    "                    joint_action = (opponent_action, action)\n",
    "                joint_action_index = game.action_space_index[joint_action]\n",
    "                score = q_opponent.forward(state)[0][joint_action_index]\n",
    "                if score > max_opponent_payoff:\n",
    "                    max_opponent_payoff = score\n",
    "            action_scores.append(max_opponent_payoff)\n",
    "        return np.array(action_scores)\n",
    "\n",
    "    def solution_policy(self, agent_id, state, game, q_models):\n",
    "        vals = np.array(self.opponent_max_values(agent_id, state, game, q_models))\n",
    "        return softmax(-vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Equilibrio de Nash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NashSolutionConcept(SolutionConcept):\n",
    "    def generate_others_actions(self, fixed_agent_id, num_agents, num_actions):\n",
    "        # Lista con las estrategias que pueden seguir los demás agentes\n",
    "        strategies_minus_me = [range(num_actions) if i != fixed_agent_id else [None]\n",
    "                               for i in range(num_agents)]\n",
    "        # itertools.product nos da el producto cartesiano\n",
    "        return list(itertools.product(*strategies_minus_me))\n",
    "\n",
    "    def calculate_best_responses(self, agent_id, scalar_state, game, q_models):\n",
    "        state = one_hot(scalar_state, game.num_states)\n",
    "        others_joint_actions = self.generate_others_actions(agent_id, game.num_agents, game.num_actions)\n",
    "        best_responses = []\n",
    "        for joint_action in others_joint_actions:\n",
    "            max_payoff = float('-inf')\n",
    "            best_response = None\n",
    "            for action in range(game.num_actions):\n",
    "                joint_action_copy = list(joint_action)\n",
    "                joint_action_copy[agent_id] = action\n",
    "                full_joint_action = tuple(joint_action_copy)\n",
    "                joint_action_index = game.action_space.index(full_joint_action)\n",
    "                q_state = q_models[agent_id].forward(state)[0]\n",
    "                payoff = q_state[joint_action_index]\n",
    "                if payoff > max_payoff:\n",
    "                    max_payoff = payoff\n",
    "                    best_response = full_joint_action\n",
    "            best_responses.append(best_response)\n",
    "        return set(best_responses)\n",
    "\n",
    "    def find_nash_equilibria(self, state, game, q_models):\n",
    "        best_responses = [self.calculate_best_responses(i, state, game, q_models) for i in range(game.num_agents)]\n",
    "        return list(set.intersection(*best_responses))\n",
    "\n",
    "    def solution_policy(self, agent_id, state, game, q_models):\n",
    "        nash_equilibria = self.find_nash_equilibria(state, game, q_models)\n",
    "        if len(nash_equilibria) > 0:\n",
    "            equilibrium = nash_equilibria[0]\n",
    "            probs = np.zeros(game.num_actions)\n",
    "            probs[equilibrium[agent_id]] = 1\n",
    "            return probs\n",
    "        else:\n",
    "            uniform_distribution = [1] * game.num_actions\n",
    "            return uniform_distribution / np.sum(uniform_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Bienestar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class WelfareSolutionConcept(SolutionConcept):\n",
    "    def find_welfare_maximizing_solutions(self, state, game, q_models):\n",
    "        joint_actions = list(itertools.product(range(game.num_actions), repeat=game.num_agents))\n",
    "        welfare_values = []\n",
    "\n",
    "        for joint_action in joint_actions:\n",
    "            welfare = self.calculate_welfare(joint_action, state, game, q_models)\n",
    "            welfare_values.append((welfare, joint_action))\n",
    "\n",
    "        max_welfare = max(welfare_values, key=lambda x: x[0])[0]\n",
    "        welfare_maximizing_solutions = [action for welfare, action in welfare_values if welfare == max_welfare]\n",
    "\n",
    "        return welfare_maximizing_solutions\n",
    "\n",
    "    def calculate_welfare(self, joint_action, state, game, q_models):\n",
    "        welfare = 0\n",
    "        joint_action_index = game.action_space_index[joint_action]\n",
    "        for agent_id in range(game.num_agents):\n",
    "            q_state = q_models[agent_id].forward(one_hot(state, game.num_states))[0]\n",
    "            welfare += q_state[joint_action_index]\n",
    "        return welfare\n",
    "\n",
    "    def solution_policy(self, agent_id, state, game, q_models):\n",
    "        welfare_solutions = self.find_welfare_maximizing_solutions(state, game, q_models)\n",
    "        num_solutions = len(welfare_solutions)\n",
    "\n",
    "        if num_solutions > 0:\n",
    "            # Initialize probability distribution for the agent's actions\n",
    "            probs = np.zeros(game.num_actions)\n",
    "\n",
    "            # Calculate the probability assigned to each action\n",
    "            for solution in welfare_solutions:\n",
    "                probs[solution[agent_id]] += 1 / num_solutions\n",
    "\n",
    "            return probs\n",
    "        else:\n",
    "            uniform_distribution = np.ones(game.num_actions)\n",
    "            return uniform_distribution / np.sum(uniform_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "A continuación definimos una función para entrenar basada en el código de la sesión anterior, parametrizable con un entorno, un modelo de juego, los algoritmos para cada agente y una función para convertir observaciones en estados. Añadimos también una función para convertir observaciones en el estado único para juegos en forma normal, que nos servirá para Rock, Paper, Scissors y el dilema del prisionero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normal_form_obs_to_state(observation):\n",
    "    # Sólo un estado (juego en forma normal)\n",
    "    return 0\n",
    "\n",
    "def train(env, game, f_obs_to_state, algorithms):\n",
    "    cumulative_rewards = [[0, 0]]\n",
    "    actions_played = [[], []]\n",
    "    all_agents = range(game.num_agents)\n",
    "\n",
    "    observations, _ = env.reset()\n",
    "    states = [f_obs_to_state(observations[f\"player_{i}\"]) for i in all_agents]\n",
    "    while env.agents:\n",
    "        # Selección de acción conjunta\n",
    "        joint_action = tuple([algorithms[i].select_action(states[i]) for i in all_agents])\n",
    "        [actions_played[i].append(joint_action[i]) for i in all_agents]\n",
    "        pettingzoo_joint_action = {f\"player_{i}\": joint_action[i] for i in all_agents}\n",
    "\n",
    "        # Actualización del entorno\n",
    "        observations, rewards, terminations, truncations, infos = env.step(pettingzoo_joint_action)\n",
    "\n",
    "        # Aprendizaje\n",
    "        observations = [observations[f\"player_{i}\"] for i in all_agents]\n",
    "        rewards = [rewards[f\"player_{i}\"] for i in all_agents]\n",
    "        new_states = [f_obs_to_state(observations[i]) for i in all_agents]\n",
    "        [algorithms[i].learn(joint_action, rewards, states[i], new_states[i])\n",
    "         for i in all_agents]\n",
    "        cumulative_rewards.append([cumulative_rewards[-1][i] + rewards[i] for i in all_agents])\n",
    "\n",
    "        # Actualizamos estado\n",
    "        states = new_states\n",
    "\n",
    "    return cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Experimento: Rock, Paper, Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_rps_2_agents(solution_concept, num_turns, gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=3)\n",
    "    algorithm_player_0 = JALGTNN(0, game_model, solution_concept,\n",
    "                                 gamma=gammas[0], alpha=alphas[0],\n",
    "                                 epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = JALGTNN(1, game_model, solution_concept,\n",
    "                                 gamma=gammas[1], alpha=alphas[1],\n",
    "                                 epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = rps_v2.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "    cumulative_rewards, actions_played = train(env, game_model, normal_form_obs_to_state,\n",
    "                                               [algorithm_player_0, algorithm_player_1])\n",
    "    env.close()\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Prueba diferentes combinaciones de soluciones de concepto y número de turnos para comprobar el comportamiento de JAL-GT con redes neuronales. Puedes abrir en otra pestaña el notebook de la sesión anterior para comparar el rendimiento tanto en optimalidad de política como en eficiencia temporal.**\n",
    "\n",
    "**¿Qué interpretación le das a los resultados en convergencia de cada solución de concepto?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played = \\\n",
    "        train_rps_2_agents(solution_concept=ParetoSolutionConcept(),\n",
    "                           num_turns=5, gammas=[0.95, 0.95], alphas=[0.01, 0.01],\n",
    "                           epsilons=[0.2, 0.2], seeds=[0, 1])\n",
    "\n",
    "# Recompensa acumulada. Debería ser [0, 0] en el infinito, si las estrategias son óptimas\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_models[0].forward(one_hot(0, game_model.num_states))[0]))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_models[1].forward(one_hot(0, game_model.num_states))[0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_models[0].forward(one_hot(0, game_model.num_states))[0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_models[1].forward(one_hot(0, game_model.num_states))[0]))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_models[0]\n",
    "                           .forward(one_hot(0, game_model.num_states)))\n",
    "                  .reshape((len(RPS_CHOICES), len(RPS_CHOICES))),\n",
    "                  index=RPS_CHOICES, columns=RPS_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"loss\"], \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Antes de cerrar, fijaos en la estructura de la red neuronal. Estamos usando un total de 121 neuronas en las tres capas. Para espacios de estados con mucha complejidad, puede ser un tamaño adecuado, pero para estos problemas en forma normal, no hace falta (de hecho, sólo tenemos un estado). **Prueba a modificar la estructura para encontrar un tamaño lo suficientemente pequeño como para que los resultados de los experimentos no se resientan.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Experimento: dilema del prisionero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "De nuevo, este es el código del entorno del dilema del prisionero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import prisoners_dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Código de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_pd(solution_concept, num_turns, gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=2)\n",
    "    algorithm_player_0 = JALGTNN(0, game_model, solution_concept,\n",
    "                                 gamma=gammas[0], alpha=alphas[0],\n",
    "                                 epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = JALGTNN(1, game_model, solution_concept,\n",
    "                                 gamma=gammas[1], alpha=alphas[1],\n",
    "                                 epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = prisoners_dilemma.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "    cumulative_rewards, actions_played = train(env, game_model, normal_form_obs_to_state,\n",
    "                                               [algorithm_player_0, algorithm_player_1])\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played = \\\n",
    "    train_pd(solution_concept=NashSolutionConcept(),\n",
    "             num_turns=5000, gammas=[0.95, 0.95], alphas=[0.01, 0.01],\n",
    "             epsilons=[0.2, 0.2], seeds=[0, 1])\n",
    "\n",
    "# Recompensa acumulada\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_models[0].forward(one_hot(0, game_model.num_states))))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_models[1].forward(one_hot(0, game_model.num_states))))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_models[0].forward(one_hot(0, game_model.num_states))))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_models[1].forward(one_hot(0, game_model.num_states))))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_models[0]\n",
    "                           .forward(one_hot(0, game_model.num_states)))\n",
    "                  .reshape((len(PD_CHOICES), len(PD_CHOICES))),\n",
    "                  index=PD_CHOICES, columns=PD_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"loss\"], \"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
