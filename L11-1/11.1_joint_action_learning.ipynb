{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install rlcard pettingzoo seaborn matplotlib numpy pandas pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from pettingzoo.classic import rps_v2\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombres de las acciones de los entornos que veremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPS_CHOICES = [\"Rock\", \"Paper\", \"Scissors\"]\n",
    "PD_CHOICES = [\"Cooperate\", \"Defect\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_array(ar):\n",
    "    return np.array_str(np.array(ar), precision=2, suppress_small=True)\n",
    "\n",
    "def draw_history(history, title):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(history) + 1), title: history})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y=title, data=data)\n",
    "\n",
    "    plt.title(title + ' Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entorno: Piedra, papel, tijera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "En clase de teoría vimos brevemente el juego de piedra, papel, tijera como ejemplo de entorno sin equilibrios puros. Vamos a utilizarlo para empezar a ver algoritmos de aprendizaje por refuerzo multiagente. Los detalles sobre este entorno se pueden ver en la [documentación](https://pettingzoo.farama.org/environments/classic/rps/) de la libreria PettingZoo, análoga a Gymnasium pero para entornos multiagente.\n",
    "\n",
    "En esta sesión, para simplificar la implementación de los algoritmos básicos, asumiremos las siguientes premisas:\n",
    "\n",
    "* Hay observabilidad total\n",
    "* Sólo hay un estado (vamos a ver únicamente juegos en forma normal en esta sesión)\n",
    "* Los agentes actúan a la vez (en paralelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos agentes aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "En primer lugar vamos a ver cómo inicializar el entorno y ejecutarlo con agentes totalmente aleatorios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_rps = rps_v2.parallel_env(max_cycles=5, render_mode=\"human\")  # Poned render_mode=None si la UI os da problemas\n",
    "env_rps.reset()\n",
    "total_rewards = [0, 0]\n",
    "turn = 1\n",
    "\n",
    "while env_rps.agents:\n",
    "    print(f\"Turn {turn}\")\n",
    "    actions = {agent: env_rps.action_space(agent).sample() for agent in env_rps.agents}\n",
    "    observations, rewards, terminations, truncations, infos = env_rps.step(actions)\n",
    "    print(f\"Player 0: plays {RPS_CHOICES[actions['player_0']]}, observes: {RPS_CHOICES[observations['player_0']]}, gains: {rewards['player_0']}\")\n",
    "    print(f\"Player 1: plays {RPS_CHOICES[actions['player_1']]}, observes: {RPS_CHOICES[observations['player_1']]}, gains: {rewards['player_1']}\")\n",
    "    total_rewards[0] += rewards['player_0']\n",
    "    total_rewards[1] += rewards['player_1']\n",
    "    turn += 1\n",
    "print(f\"Cumulative rewards: {total_rewards[0]}, {total_rewards[1]}\")\n",
    "env_rps.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo: Joint Action Learning with Game Theory (JAL-GT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo JAL-GT (Joint Action Learning with Game Theory) es un algoritmo de diferencias temporales que utiliza Q-Learning para aproximar los valores estado-acción de todos los agentes. La política a seguir la marcará el concepto de solución, usando la tabla de valor Q como si fuera una matriz de recompensas (de ahí el \"with Game Theory\" del nombre del algoritmo).\n",
    "\n",
    "El pseudocódigo del algoritmo es el siguiente ([Multi-Agent Reinforcement Learning, Albrecht et al. 2024, p.120](https://www.marl-book.com/download/)):\n",
    "\n",
    "<div>\n",
    "<img src=\"Algorithm7.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "El símbolo $\\Gamma$ (representando el concepto de \"juego\") se refiere a la matriz de recompensas que se calcula en base a partir de los valores de la función Q:\n",
    "\n",
    "<div>\n",
    "<img src=\"gamma.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Las diferencias con respecto al algoritmo Q-Learning que vimos en sesiones anteriores son las siguientes:\n",
    "\n",
    "* Se construye una función de valor Q para cada agente, no sólo para el agente que está aprendiendo la política.\n",
    "* La política no se calcula como $max_{a' \\in A} Q(s'|a')$ sino que se calcula resolviendo el juego $\\Gamma$ usando el concepto de solución elegido (e.g. equilibrio de Nash, minimax, óptimo de Pareto, etc.).\n",
    "* La función de error (TD target) de diferencia temporal en este caso tampoco se calcula en base a $max_{a' \\in A} Q(s'|a')$ sino que se calcula a partir del valor Q en el punto de equilibrio encontrado a partir del concepto de solución.\n",
    "\n",
    "Para implementar el algoritmo, por lo tanto, necesitaremos los siguientes elementos:\n",
    "* Un modelo de juego\n",
    "* Un concepto de solución\n",
    "* El algoritmo en sí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos un modelo de juego como una tupla de número de agentes, un número de estados y un espacio de acciones. Podríamos añadir otros elementos, como por ejemplo un histórico de estados, de recompensas, de acciones o de observaciones pero para los juegos que vamos a ver en esta sesión no son necesarios todavía.\n",
    "\n",
    "El espacio de acciones que genera la constructora es el espacio de **acciones conjuntas**: el conjunto de combinaciones de acciones para cada agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameModel:\n",
    "    def __init__(self, num_agents, num_states, num_actions):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = self.generate_action_space()\n",
    "\n",
    "    def generate_action_space(self):\n",
    "        actions_by_players = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            actions_by_players.append(range(self.num_actions))\n",
    "        all_joint_actions = itertools.product(*actions_by_players)\n",
    "        return [tuple(l) for l in all_joint_actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto de solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El concepto de solución selecciona una estrategia pura _óptima_ para un agente determinado en un juego (matriz de recompensas). Qué estrategia se considera óptima es parte de la definición de este concepto de solución. Por ejemplo, si el concepto es la Pareto-eficiencia el agente debería escoger una acción individual que sea parte de la acción conjunta que sea óptimo de Pareto del juego.\n",
    "\n",
    "Definimos un método abstract `solution_policy` que retorna una estrategia, potencialmente mixta, como una distribución de probabilidad sobre las acciones individuales, representando la calidad de cada acción según el concepto de solución. Además definimos un método para depurar la solución que usaremos más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolutionConcept(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def solution_policy(self, agent_id, state, game, q_table):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def debug(self, agent_id, state, game, q_table):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar ahora nuestro primer concepto de solución: Minimax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class MinimaxSolutionConcept(SolutionConcept):\n",
    "    def opponent_max_values(self, agent_id, state, game, q_table):\n",
    "        action_scores = []\n",
    "        for action in range(game.num_actions):\n",
    "            max_opponent_payoff = float('-inf')\n",
    "            for opponent_action in range(game.num_actions):\n",
    "                if agent_id == 0:  # Suponemos sólo dos agentes\n",
    "                    joint_action = (action, opponent_action)\n",
    "                else:\n",
    "                    joint_action = (opponent_action, action)\n",
    "                joint_action_index = game.action_space.index(joint_action)\n",
    "                score = q_table[1 - agent_id][state][joint_action_index]\n",
    "                if score > max_opponent_payoff:\n",
    "                    max_opponent_payoff = score\n",
    "            action_scores.append(max_opponent_payoff)\n",
    "        return np.array(action_scores)\n",
    "\n",
    "    def solution_policy(self, agent_id, state, game, q_table):\n",
    "        vals = np.array(self.opponent_max_values(agent_id, state, game, q_table))\n",
    "        return softmax(-vals)\n",
    "\n",
    "    def debug(self, agent_id, state, game, q_table):\n",
    "        vals = self.opponent_max_values(agent_id, state, game, q_table)\n",
    "        print(f\"La política minimax del agente {agent_id} minimiza los máximos valores del oponente, que son: {vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear primero una clase abstracta para implementar diferentes algoritmos de aprendizaje por refuerzo multiagente. Definimos tres métodos abstractos:\n",
    "\n",
    "* `learn`, para actualizar la tabla de valor Q\n",
    "* `select_action`, para elegir una acción\n",
    "* `explain`, como utilidad para extraer información relevante sobre la ejecución del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARLAlgorithm(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, joint_action, rewards, next_state: int, observations):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def explain(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el algoritmo como subclase de `MARLAlgorithm`. La constructora recibe el identificador del agente que usa el algoritmo para aprender, el modelo de juego y el concepto de solución. Aparte, se añaden algunos parámetros familiares: $\\gamma$ (factor de descuento), $\\alpha$ (tasa de aprendizaje), $\\epsilon$ (coeficiente de exploración), y una semilla para el generador de números aleatorios.\n",
    "\n",
    "La tabla `q_table` tiene las siguientes dimensiones: $N \\times S \\times AS$, donde $AS$ es el conjunto de acciones conjuntas y por lo tanto tiene tamaño $|N|^{|A|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JALGT(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel, solution_concept: SolutionConcept,\n",
    "                 gamma=0.95, alpha=0.5, epsilon=0.2, seed=42):\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.solution_concept = solution_concept\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = random.Random(seed)\n",
    "        # Q: N x S x AS\n",
    "        self.q_table = [[[0 for _ in range(len(self.game.action_space))]\n",
    "                         for _ in range(self.game.num_states)]\n",
    "                        for _ in range(self.game.num_agents)]\n",
    "        # Política conjunta por defecto: distribución uniforme respecto\n",
    "        # de las acciones conjuntas, para cada acción (pi(a | s))\n",
    "        self.joint_policy = [[[1/self.game.num_actions] * self.game.num_actions\n",
    "                              for _ in range(self.game.num_states)]\n",
    "                             for _ in range(self.game.num_agents)]\n",
    "        self.metrics = {\"td_error\": []}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el cálculo del valor de un estado para un agente, vamos a seguir la fórmula ([Multi-Agent Reinforcement Learning, Albrecht et al. 2024, p.120](https://www.marl-book.com/download/)):\n",
    "\n",
    "$Value_i(\\Gamma_{s'})=\\sum_{a\\in A} \\Gamma_{s',i}(a)\\pi^*_{s'} (a)$\n",
    "\n",
    "Es decir, dado el juego $\\Gamma$ definido a partir de ver la tabla de valor Q como si fuera una matriz de recompensas, el valor de un estado-acción $(s, a_i)$ para el agente $i$ es la utilidad esperada para $i$ según el conocimiento actual del agente: la suma de la recompensa para $i$, en $s$, para cada acción conjunta $a$ donde el agente realiza $a_i$, multiplicada por la probabilidad de realizar la acción conjunta $a$ según la política conjunta: $\\pi^{*}(a|s)$.\n",
    "\n",
    "Recordad, como hemos visto en teoría, que la política conjunta es el productorio de las políticas individuales, aquí implementado con `np.prod()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JALGT(JALGT):\n",
    "    def value(self, agent_id, state):\n",
    "        value = 0\n",
    "        for idx, joint_action in enumerate(self.game.action_space):\n",
    "            payoff = self.q_table[agent_id][state][idx]\n",
    "            joint_probability = np.prod([self.joint_policy[i][state][joint_action[i]]\n",
    "                                         for i in range(self.game.num_agents)])\n",
    "            value += payoff * joint_probability\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `learn` implementa las líneas 8 y 9 del pseudocódigo del algoritmo. Para no calcularlas constantemente, añadimos un método `update_policy(agent_id)` que llamamos justo después de actualizar la función Q de cada agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JALGT(JALGT):\n",
    "    def update_policies(self, agent_id):\n",
    "        self.joint_policy[agent_id] =\\\n",
    "            [self.solution_concept.solution_policy(agent_id, state, self.game, self.q_table)\n",
    "             for state in range(self.game.num_states)]\n",
    "\n",
    "    def learn(self, joint_action, rewards, next_state: int, observations):\n",
    "        joint_action_index = self.game.action_space.index(joint_action)\n",
    "        for agent_id in range(self.game.num_agents):\n",
    "            agent_reward = rewards[agent_id]\n",
    "            agent_game_value = self.value(agent_id, next_state)\n",
    "            agent_q_value = self.q_table[agent_id][next_state][joint_action_index]\n",
    "            td_target = agent_reward + self.gamma * agent_game_value - agent_q_value\n",
    "            self.q_table[agent_id][next_state][joint_action_index] += self.alpha * td_target\n",
    "            self.update_policies(agent_id)\n",
    "            # Guardamos el error de diferencia temporal para estadísticas posteriores\n",
    "            self.metrics['td_error'].append(td_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "El método `select_action` implementa las líneas 5 y 6 del pseudocódigo. En este caso, la solución no depende directamente del valor, como en el caso del algoritmo original de Q-Learning ($\\arg\\max_{a\\in A} Q^*(s,a)$), sino que se obtiene a partir del concepto de solución.\n",
    "\n",
    "Aparte, también añadimos un método auxiliar para depurar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALGT(JALGT):\n",
    "    def solve(self, agent_id, state):\n",
    "        return self.solution_concept.solution_policy(agent_id, state, self.game, self.q_table)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(range(self.game.num_actions))\n",
    "        else:\n",
    "            return np.argmax(self.solve(self.agent_id, state))\n",
    "\n",
    "    def explain(self):\n",
    "        return self.solution_concept.debug(self.agent_id, 0, self.game, self.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experimento 1: dos agentes entrenando en paralelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Ahora vamos a utilizar estas clases y la librería de PettingZoo para inicializar el entorno y los algoritmos, y entrenar. Definimos una función para realizar los experimentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rps_2_agents(solution_concept, num_turns, gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=3)\n",
    "    algorithm_player_0 = JALGT(0, game_model, solution_concept,\n",
    "                               gamma=gammas[0], alpha=alphas[0], epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = JALGT(1, game_model, solution_concept,\n",
    "                               gamma=gammas[1], alpha=alphas[1], epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = rps_v2.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "    env.reset()\n",
    "\n",
    "    # Sólo tenemos un estado: es un juego en forma normal que siempre tiene la misma matriz de recompensa\n",
    "    normal_form_state = 0\n",
    "    cumulative_rewards = [[0, 0]]\n",
    "    actions_played = [[], []]\n",
    "\n",
    "    while env.agents:\n",
    "        action_0 = algorithm_player_0.select_action(normal_form_state)\n",
    "        action_1 = algorithm_player_1.select_action(normal_form_state)\n",
    "        actions = {'player_0': action_0, 'player_1': action_1}\n",
    "        actions_played[0].append(action_0)\n",
    "        actions_played[1].append(action_1)\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        tuple_observations = (observations['player_0'], observations['player_1'])\n",
    "        tuple_rewards = (rewards['player_0'], rewards['player_1'])\n",
    "        algorithm_player_0.learn((action_0, action_1), tuple_rewards,\n",
    "                                 normal_form_state, tuple_observations)\n",
    "        algorithm_player_1.learn((action_0, action_1), tuple_rewards,\n",
    "                                 normal_form_state, tuple_observations)\n",
    "        cumulative_rewards.append([cumulative_rewards[-1][0] + tuple_rewards[0],\n",
    "                                   cumulative_rewards[-1][1] + tuple_rewards[1]])\n",
    "\n",
    "    env.close()\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Configuramos, ejecutamos y analizamos resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played =\\\n",
    "    train_rps_2_agents(solution_concept=MinimaxSolutionConcept(),\n",
    "                       num_turns=10, gammas=[0.95, 0.95], alphas=[0.1, 0.1],\n",
    "                       epsilons=[0.2, 0.2], seeds=[0, 0])\n",
    "\n",
    "# Recompensa acumulada. Debería ser [0, 0] en el infinito, si las estrategias son óptimas\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_table[0][0]).reshape((len(RPS_CHOICES), len(RPS_CHOICES))),\n",
    "                  index=RPS_CHOICES, columns=RPS_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Explicación de la aplicación del concepto de solución:\n",
    "algorithm_player_0.explain()\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history([cumulative_rewards[x][0] for x in range(len(cumulative_rewards))],\n",
    "             \"Recompensas acumuladas, agente 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error (agente 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**¿Diríais que los agentes han entrenado bien? Si no es así, ¿qué creeis que ha pasado?**\n",
    "\n",
    "**Si es necesario, repasad cómo funciona el algoritmo y comprobad con detenimiento qué valores hemos puesto a los parámetros por defecto.**\n",
    "\n",
    "Si os ayuda, podéis inspeccionar las acciones que los agentes han tomado durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Primeras jugadas:\n",
    "for i in range(5):\n",
    "    print(f\"0 juega {actions_played[0][i]}, 1 juega {actions_played[1][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Una vez consigáis que empiece a entrenar, id comprobando el valor de las políticas. **Para agentes que se guían por el valor Minimax, ¿qué valor deberían tener las probabilidades de la política en este juego? Modificad parámetros del experimento hasta que consigáis aproximar la política a valores muy cercanos al teórico.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experimento 2: Agente vs Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Por mucho que hayáis conseguido aproximar la política bien, muy probablemente los valores Q no se ajusten perfectamente a las recompensas que da el entorno. Esto generalmente es debido a que cuando dos o más agentes están entrenando simultáneamente, se producen sesgos en el comportamiento basados en cualquier pequeña variación en las valoraciones inferidas de los otros agentes. Si el valor Minimax nos dice que el contrario debería valorar más Rock (y podría ser simplemente porque estamos en el turno 2 y en el turno 1 el contrario ganó con Rock) sabemos que deberíamos evitar Scissors, por lo que estamos sesgando la decisión inmediata. A la larga, esto se puede traducir en patrones raros donde una misma jugada se repite muchas veces hasta que el contrario se adapta, dando lugar a celdas de la tabla Q que están mejor calculadas que otras (simplemente por ley de los grandes números).\n",
    "\n",
    "Vamos a ver qué pasa si entrenamos a nuestro agente JAL-GT a competir contra un agente totalmente aleatorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomAlgorithm(MARLAlgorithm):\n",
    "    def __init__(self, game, seed=42):\n",
    "        self.game = game\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    def learn(self, joint_action, rewards, next_state: int, observations):\n",
    "        pass\n",
    "\n",
    "    def explain(self):\n",
    "        print(\"Random algorithm\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        return self.rng.choice(range(self.game.num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_rps_vs_random(solution_concept, num_turns, gamma, alpha, epsilon, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=3)\n",
    "    algorithm_player_0 = JALGT(0, game_model, solution_concept,\n",
    "                               gamma=gamma, alpha=alpha, epsilon=epsilon, seed=seeds[0])\n",
    "    algorithm_player_1 = RandomAlgorithm(game_model, seed=seeds[1])\n",
    "    env = rps_v2.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "    env.reset()\n",
    "\n",
    "    # Sólo tenemos un estado: es un juego en forma normal que siempre tiene la misma matriz de recompensa\n",
    "    normal_form_state = 0\n",
    "    cumulative_rewards = [[0, 0]]\n",
    "    actions_played = [[], []]\n",
    "\n",
    "    while env.agents:\n",
    "        action_0 = algorithm_player_0.select_action(normal_form_state)\n",
    "        action_1 = algorithm_player_1.select_action(normal_form_state)\n",
    "        actions = {'player_0': action_0, 'player_1': action_1}\n",
    "        actions_played[0].append(action_0)\n",
    "        actions_played[1].append(action_1)\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        tuple_observations = (observations['player_0'], observations['player_1'])\n",
    "        tuple_rewards = (rewards['player_0'], rewards['player_1'])\n",
    "        algorithm_player_0.learn((action_0, action_1), tuple_rewards,\n",
    "                                 normal_form_state, tuple_observations)\n",
    "        algorithm_player_1.learn((action_0, action_1), tuple_rewards,\n",
    "                                 normal_form_state, tuple_observations)\n",
    "        cumulative_rewards.append([cumulative_rewards[-1][0] + tuple_rewards[0],\n",
    "                                   cumulative_rewards[-1][1] + tuple_rewards[1]])\n",
    "\n",
    "    env.close()\n",
    "    return game_model, algorithm_player_0, cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, cumulative_rewards, actions_played =\\\n",
    "    train_rps_vs_random(solution_concept=MinimaxSolutionConcept(),\n",
    "                        num_turns=15, gamma=0.95, alpha=0.1, epsilon=0.2, seeds=[0, 0])\n",
    "\n",
    "# Recompensa acumulada. Debería ser [0, 0] en el infinito, si las estrategias son óptimas\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_table[0][0]).reshape((len(RPS_CHOICES), len(RPS_CHOICES))),\n",
    "                  index=RPS_CHOICES, columns=RPS_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Explicación de la aplicación del concepto de solución:\n",
    "algorithm_player_0.explain()\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Modificad los parámetros del experimento para intentar que la matriz de recompensas esté cerca de los valores teóricos.**\n",
    "\n",
    "**¿Qué conclusiones sacáis de este análisis? ¿Qué modelo creéis que converge antes en unos óptimos reales y por qué?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Experimento 3: otro concepto de solución (equilibrio de Nash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Hemos usado el concepto de solución Minimax en el ejemplo anterior. Vamos a implementar otro concepto más: el equilibrio de Nash. Para esta implementación, vamos a replicar el método que hemos usado en clases de problemas para resolver este tipo de juegos con este concepto: el cruce de mejores respuestas. Vamos paso a paso.\n",
    "\n",
    "Para empezar, vamos a definir una función para obtener todas las posibles estrategias combinadas de todos los agentes menos uno. Esto nos permitirá identificar más adelante el conjunto de acciones _casi-conjuntas_ para las que buscar mejores respuestas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NashSolutionConcept(SolutionConcept):\n",
    "    def generate_others_actions(self, fixed_agent_id, num_agents, num_actions):\n",
    "        # Lista con las estrategias que pueden seguir los demás agentes\n",
    "        strategies_minus_me = [range(num_actions) if i != fixed_agent_id else [None]\n",
    "                               for i in range(num_agents)]\n",
    "        # itertools.product nos da el producto cartesiano\n",
    "        return list(itertools.product(*strategies_minus_me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "A partir de una lista de acciones _casi-conjuntas_, para un agente y un estado, vamos a buscar la mejor respuesta para cada una. Iteramos sobre esta lista, encontrando la acción para la que nuestra recompensa estimada (a partir de la función Q) es más alta. Guardamos todas las mejores respuestas en un conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NashSolutionConcept(NashSolutionConcept):\n",
    "    def calculate_best_responses(self, agent_id, state, game, q_table):\n",
    "        others_joint_actions = self.generate_others_actions(agent_id, game.num_agents, game.num_actions)\n",
    "        best_responses = []\n",
    "        for joint_action in others_joint_actions:\n",
    "            max_payoff = float('-inf')\n",
    "            best_response = None\n",
    "            for action in range(game.num_actions):\n",
    "                joint_action_copy = list(joint_action)\n",
    "                joint_action_copy[agent_id] = action\n",
    "                full_joint_action = tuple(joint_action_copy)\n",
    "                joint_action_index = game.action_space.index(full_joint_action)\n",
    "                payoff = q_table[agent_id][state][joint_action_index]\n",
    "                if payoff > max_payoff:\n",
    "                    max_payoff = payoff\n",
    "                    best_response = full_joint_action\n",
    "            best_responses.append(best_response)\n",
    "        return set(best_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Encontrar los equilibrios de Nash es tan sencillo como generar todos los conjuntos de mejores respuestas para todos los agentes y calcular la intersección (_el cruce_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NashSolutionConcept(NashSolutionConcept):\n",
    "    def find_nash_equilibria(self, state, game, q_table):\n",
    "        best_responses = [self.calculate_best_responses(i, state, game, q_table) for i in range(game.num_agents)]\n",
    "        return list(set.intersection(*best_responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Con una lista de equilibrios de Nash, podríamos seguir varios métodos para obtener una política. Por ejemplo, hay métodos para calcular una estrategia mixta (probabilidades arbitrarias entre 0 y 1), o podríamos buscar el equilibrio que nos beneficie más.\n",
    "\n",
    "Por simplicidad, en esta implementación simplemente nos vamos a quedar con el primer equilibrio que tengamos en la respuesta y usar la acción del agente en esa acción conjunta para poner su probabilidad a 1. Si no hay equilibrios de Nash, retornamos una política con una distribución de probabilidad uniforme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NashSolutionConcept(NashSolutionConcept):\n",
    "    def solution_policy(self, agent_id, state, game, q_table):\n",
    "        nash_equilibria = self.find_nash_equilibria(state, game, q_table)\n",
    "        if len(nash_equilibria) > 0:\n",
    "            equilibrium = nash_equilibria[0]\n",
    "            probs = np.zeros(game.num_actions)\n",
    "            probs[equilibrium[agent_id]] = 1\n",
    "            return probs\n",
    "        else:\n",
    "            uniform_distribution = [1] * game.num_actions\n",
    "            return uniform_distribution / np.sum(uniform_distribution)\n",
    "\n",
    "    def debug(self, agent_id, state, game, q_table):\n",
    "        best_responses = [self.calculate_best_responses(i, state, game, q_table) for i in range(game.num_agents)]\n",
    "        for i in range(game.num_agents):\n",
    "            print(f\"Mejores respuestas para el agente {i}: {best_responses[i]}\")\n",
    "        print(f\"El conjunto de equilibrios de Nash es: {self.find_nash_equilibria(state, game, q_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played =\\\n",
    "    train_rps_2_agents(solution_concept=NashSolutionConcept(),\n",
    "                       num_turns=15, gammas=[0.95, 0.95], alphas=[0.1, 0.1],\n",
    "                       epsilons=[0.2, 0.2], seeds=[0, 1])\n",
    "\n",
    "# Recompensa acumulada. Debería ser [0, 0] en el infinito, si las estrategias son óptimas\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_table[0][0]).reshape((len(RPS_CHOICES), len(RPS_CHOICES))),\n",
    "                  index=RPS_CHOICES, columns=RPS_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Explicación de la aplicación del concepto de solución:\n",
    "algorithm_player_0.explain()\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Interpreta qué está ocurriendo cuando aplicamos equilibrio de Nash como concepto de solución implementado de esta manera (sólo para estrategias puras, o política totalmente aleatoria si no hay). ¿Qué mejoras aplicarías?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Experimento 4: juego de intereses mixtos (dilema del prisionero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "En esta sección vamos a explorar estas dos soluciones de concepto en un juego que no es puramente competitivo, sino de intereses mixtos: el dilema del prisionero. En PettingZoo no tenemos este juego incluido como entorno, pero sí podemos usar la librería para construir uno a mano.\n",
    "\n",
    "La clase `Game` es una clase base que podéis usar para implementar juegos diferentes, como por ejemplo los de la colección de problemas (Teoría de Juegos). Este código está copiado del repositorio: [https://github.com/tianyu-z/pettingzoo_dilemma_envs](https://github.com/tianyu-z/pettingzoo_dilemma_envs). Lo tenéis en el fichero `prisoners_dilemma.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import prisoners_dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_pd(solution_concept, num_turns, gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=2)\n",
    "    algorithm_player_0 = JALGT(0, game_model, solution_concept, gamma=gammas[0], alpha=alphas[0], epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = JALGT(1, game_model, solution_concept, gamma=gammas[1], alpha=alphas[1], epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = prisoners_dilemma.pd_env(max_cycles=1, render_mode=\"ansi\")\n",
    "    normal_form_state = 0\n",
    "    cumulative_rewards = [[0, 0]]\n",
    "    actions_played = [[], []]\n",
    "\n",
    "    for i in range(num_turns):\n",
    "        env.reset()\n",
    "        while env.agents:\n",
    "            action_0 = algorithm_player_0.select_action(normal_form_state)\n",
    "            action_1 = algorithm_player_1.select_action(normal_form_state)\n",
    "            actions = {'player_0': action_0, 'player_1': action_1}\n",
    "\n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            tuple_observations = (observations['player_0'], observations['player_1'])\n",
    "            tuple_rewards = (rewards['player_0'], rewards['player_1'])\n",
    "            algorithm_player_0.learn((action_0, action_1), tuple_rewards,\n",
    "                                     normal_form_state, tuple_observations)\n",
    "            algorithm_player_1.learn((action_0, action_1), tuple_rewards,\n",
    "                                     normal_form_state, tuple_observations)\n",
    "\n",
    "            # Recolección de métricas\n",
    "            actions_played[0].append(action_0)\n",
    "            actions_played[1].append(action_1)\n",
    "            cumulative_rewards.append([cumulative_rewards[-1][0] + tuple_rewards[0],\n",
    "                                       cumulative_rewards[-1][1] + tuple_rewards[1]])\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played = \\\n",
    "    train_pd(solution_concept=NashSolutionConcept(),\n",
    "             num_turns=15, gammas=[0.95, 0.95], alphas=[0.1, 0.1],\n",
    "             epsilons=[0.2, 0.2], seeds=[0, 1])\n",
    "\n",
    "# Recompensa acumulada\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 0 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_table[0][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 0, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "print(\"Valores Q calculados por el agente 1 sobre el agente 1, indexados por acciones conjuntas:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_table[1][0]))\n",
    "\n",
    "# Convertimos en un dataframe para mostrar la matriz de recompensas para el agente 0:\n",
    "df = pd.DataFrame(np.array(algorithm_player_0.q_table[0][0]).reshape((len(PD_CHOICES), len(PD_CHOICES))),\n",
    "                  index=PD_CHOICES, columns=PD_CHOICES)\n",
    "print(\"Matriz de recompensas estimadas para el agente 0:\")\n",
    "print(df)\n",
    "\n",
    "# Explicación de la aplicación del concepto de solución:\n",
    "algorithm_player_0.explain()\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {algorithm_player_0.solve(0, 0)}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {algorithm_player_0.solve(0, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Para estar seguros de si el resultado es correcto, podemos comprovar como está convergiendo el algoritmo. Por ejemplo, siguiendo la función de pérdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error (agente 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Ajusta los parámetros para conseguir que la curva de pérdida se estabilice y converja en el cero.**\n",
    "\n",
    "**Una vez conseguido, comprueba si el orden de recompensas se mantiene sobre el teórico. Puede ser que no sea así, en tal caso, ¿podrías interpretar qué ocurre analizando el algoritmo?**\n",
    "\n",
    "Una pista podría estar en la cuenta de qué acciones se han explorado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "count_coop = 0\n",
    "count_def = 0\n",
    "for i in actions_played[0]:\n",
    "    if i == 0:\n",
    "        count_coop +=1\n",
    "    else:\n",
    "        count_def += 1\n",
    "print(f\"El agente 0 ha cooperado {count_coop} veces, traicionado {count_def} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Aplica el equilibrio de Nash como concepto de solución para este entorno, experimenta e interpreta los resultados.**\n",
    "\n",
    "**Implementa una nueva solución de concepto basada en el Óptimo de Pareto y experimenta con ella.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Algoritmo: Joint Action Learning with Agent Modelling (JAL-AM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Una segunda familia de algoritmos de aprendizaje por refuerzo multiagente es el de _juego ficticio_ o modelado de agentes, cuya idea principal es la de modelar, probabilísticamente, el comportamiento (la política) de los otros agentes a partir de la observación de las acciones y las recompensas que reciben. Dada una cantidad $C(a_j)$ que representa el número de veces que el agente $j$ ha seleccionado la acción $a_j$ en el pasado, se define un modelo $\\hat{\\pi}_j$:\n",
    "\n",
    "$\\hat{\\pi}_j(a_j)=\\frac{C(a_j)}{\\sum_{a'_j} C(a'_j)}$\n",
    "\n",
    "Si tenemos en cuenta el estado:\n",
    "\n",
    "$\\hat{\\pi}_j(a_j | s)=\\frac{C(s, a_j)}{\\sum_{a'_j} C(s, a'_j)}$\n",
    "\n",
    "A partir de este modelo de agente se desarrolla el algoritmo Joint Action Learning with Agent Modelling (JAL-AM) cuyo pseudocódigo es:\n",
    "\n",
    "([Multi-Agent Reinforcement Learning, Albrecht et al. 2024, p.132](https://www.marl-book.com/download/)):\n",
    "\n",
    "<div>\n",
    "<img src=\"Algorithm8.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "En este algoritmo, sólo estamos generando la función Q para el agente que entrena. Para todos los demás, construimos un modelo (política). Para calcular el valor del estado y para obtener la política, en vez de la función $Value$ que habíamos definido en base a la matriz de recompensas como hacíamos en JAL-GT, en este algoritmo usamos la función $AV$ (action value) que aparece en el pseudocódigo:\n",
    "\n",
    "$AV_i(s, a_i) = \\sum_{a_{-i}\\in A_{-i}} Q_i(s, \\langle a_i, a_{-i}\\rangle)\\prod_{j\\neq i} \\hat{\\pi}_j (a_j | s)$\n",
    "\n",
    "Intuitivamente, esta función nos da, para un agente $i$, un estado $s$ y una determinada acción individual $a_i$, la suma del valor Q para cada acción conjunta $a$ donde $i$ realiza $a_i$, multiplicado por la probabilidad conjunta (productorio) de que los demás agentes realicen sus acciones de $a$.\n",
    "\n",
    "Una limitación de este algoritmo es que el único concepto de solución que soporta es el de _mejores respuestas_, por lo que no garantiza equilibrios ni Pareto-optimalidad. Sin embargo, los algoritmos de esta familia son bastante efectivos en la adaptación de agentes al comportamiento dinámico y no-estacionario de otros agentes presentes en el entorno.\n",
    "\n",
    "Una posible implementación de este algoritmo es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JALAM(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel,\n",
    "                 gamma=0.95, alpha=0.5, epsilon=0.2, seed=42):\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = random.Random(seed)\n",
    "        # Q: S x AS\n",
    "        self.q_table = [[0 for _ in range(len(self.game.action_space))]\n",
    "                        for _ in range(self.game.num_states)]\n",
    "        # Política conjunta por defecto: distribución uniforme respecto\n",
    "        # de las acciones conjuntas, para cada acción (pi(a | s))\n",
    "        self.joint_policy = [[[1 / self.game.num_actions] * self.game.num_actions\n",
    "                              for _ in range(self.game.num_states)]\n",
    "                             for _ in range(self.game.num_agents)]\n",
    "        # La función C(s, a)\n",
    "        self.action_counts = [[[0] * self.game.num_actions\n",
    "                               for _ in range(self.game.num_states)]\n",
    "                              for _ in range(self.game.num_agents)]\n",
    "        self.metrics = {\"td_error\": []}\n",
    "\n",
    "    def generate_joint_actions(self, fixed_agent_id, action_i):\n",
    "        joint_actions = [range(self.game.num_actions)\n",
    "                         if i != fixed_agent_id else [action_i]\n",
    "                         for i in range(self.game.num_agents)]\n",
    "        return list(itertools.product(*joint_actions))\n",
    "\n",
    "    def action_value(self, agent_id, state, action_i):\n",
    "        value = 0\n",
    "        for joint_action in self.generate_joint_actions(agent_id, action_i):\n",
    "            q_value = self.q_table[state][self.game.action_space.index(joint_action)]\n",
    "            prob = 1\n",
    "            for j in range(self.game.num_agents):\n",
    "                if j != agent_id:\n",
    "                    prob *= self.joint_policy[j][state][joint_action[j]]\n",
    "            value += q_value * prob\n",
    "        return value\n",
    "\n",
    "    def update_policy(self, agent_id, state, action):\n",
    "        self.action_counts[agent_id][state][action] += 1\n",
    "        action_counts = np.array(self.action_counts[agent_id][state])\n",
    "        new_probs = action_counts / np.sum(action_counts)\n",
    "        self.joint_policy[agent_id][state] = new_probs\n",
    "\n",
    "    def learn(self, joint_action, rewards, next_state: int, observations):\n",
    "        joint_action_index = self.game.action_space.index(joint_action)\n",
    "        for agent_id in range(self.game.num_agents):\n",
    "            agent_action = joint_action[agent_id]\n",
    "            self.update_policy(agent_id, next_state, agent_action)\n",
    "            agent_reward = rewards[agent_id]\n",
    "            agent_action_value = self.action_value(agent_id, next_state, agent_action)\n",
    "            agent_q_value = self.q_table[next_state][joint_action_index]\n",
    "            td_target = agent_reward + self.gamma * agent_action_value - agent_q_value\n",
    "            self.q_table[next_state][joint_action_index] += self.alpha * td_target\n",
    "            # Guardamos el error de diferencia temporal para estadísticas posteriores\n",
    "            self.metrics['td_error'].append(td_target)\n",
    "\n",
    "    def policy(self, state):\n",
    "        return np.argmax([self.action_value(self.agent_id, state, action_i)\n",
    "                          for action_i in range(self.game.num_actions)])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(range(self.game.num_actions))\n",
    "        else:\n",
    "            return self.policy(state)\n",
    "\n",
    "    def explain(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Utiliza este algoritmo para entrenar agentes en Rock, Paper, Scissors y en el Dilema del Prisionero. Emperimenta con los parámetros para comparar la convergencia a valores óptimos de política entre JAL-GT y JAL-AM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# A partir de aquí..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "El problema principal de estos algoritmos, en estas formas básicas que hemos implementado aquí, es que las matrices para Q en los dos algoritmos, o para la política del otro en modelado de agente, tienen dimensiones que escalan muy mal. Pensad, por ejemplo, qué pasaría si tuviéramos un espacio de estados como en el entorno (relativamente) sencillo de [Póker de PettingZoo](https://pettingzoo.farama.org/environments/classic/texas_holdem/): un vector de 72 observaciones booleanas. El espacio de estados expande a $2^{72}$, lo cual es totalmente inviable para estas implementaciones.\n",
    "\n",
    "La solución es similar a la que vimos brevemente en aprendizaje por refuerzo con la aproximación lineal en Q-Learning o en REINFORCE. En la siguiente sesión veremos cómo escalar en complejidad de entorno usando el mismo tipo de técnicas y, de paso, veremos también algoritmos de gradiente de política para multiagente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
